{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcolinmorgan/grph/blob/main/gdocs_rag_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJO2rllsnFNB"
      },
      "source": [
        "# Building RAG Chatbot with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vyYa6MtBnFNB",
        "outputId": "b4a1b60a-0b3e-4d13-ddd3-403a235cbdb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    openai \\\n",
        "    langchain \\\n",
        "    tiktoken \\\n",
        "    opensearch-py \\\n",
        "    sentence-transformers\n",
        "#     langchain_community \\\n",
        "#     langchain_text_splitters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6g6gIO8GnFNC"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "# from google.colab import userdata\n",
        "\n",
        "# OS_TOKEN = userdata.get('OS_TOKEN')\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# os.environ[\"OS_TOKEN\"] = userdata.get('OS_TOKEN')\n",
        "# chat = ChatOpenAI(\n",
        "#     openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "#     model='gpt-3.5-turbo'\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup generic OS gpy.docs index"
      ],
      "metadata": {
        "id": "hNp3CS6D0UgO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ke3NTBWJnFNI"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.document_loaders import TextLoader\n",
        "# from langchain_community.vectorstores import OpenSearchVectorSearch\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "# from langchain_text_splitters import CharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OS_TOKEN = userdata.get('OS_TOKEN')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OS_TOKEN\"] = userdata.get('OS_TOKEN')\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "# chat = ChatOpenAI(\n",
        "#     openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "#     model='gpt-3.5-turbo'\n",
        "# )"
      ],
      "metadata": {
        "id": "74sM_ZQT4Rtx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# init open-source embedding model (downloaded from hugging-face hub)\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "OxfWcXg72Rmd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings"
      ],
      "metadata": {
        "id": "kadmxrTx9wLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RWt-T7CTnFNI"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import OpenSearchVectorSearch\n",
        "\n",
        "# Init OpenSearch client connection\n",
        "docsearch = OpenSearchVectorSearch(\n",
        "     index_name=\"pygraphistry-docs\",  # TODO: use the same index-name used in the ingestion script\n",
        "     embedding_function=embeddings,\n",
        "     opensearch_url=OS_TOKEN,  # TODO: e.g. use the AWS OpenSearch domain instantiated previously\n",
        "#     http_auth=(\"<<insert_user_name>>\", \"<<insert_password>>\"),\n",
        "#     use_ssl = False,\n",
        "#     verify_certs = False,\n",
        "#     ssl_assert_hostname = False,\n",
        "#     ssl_show_warn = False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch.engine"
      ],
      "metadata": {
        "id": "AkainX7m7Iwn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can i use gfql chain?\"\n",
        "docs = docsearch.similarity_search(query, k=10)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "zCxkqKi25wDv",
        "outputId": "9d12a860-4a20-4fe6-b327-fe010a50af19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "See help(g.dbscan) or help(g.transform_dbscan) for options\n",
            "Quickly configurable\n",
            "Set visual attributes through quick data bindings and set all sorts of URL options. Check out the tutorials on colors, sizes, icons, badges, weighted clustering and sharing controls:\n",
            "  g\n",
            "    .privacy(mode='private', invited_users=[{'email': 'friend1@site.ngo', 'action': '10'}], notify=False)\n",
            "    .edges(df, 'col_a', 'col_b')\n",
            "    .edges(my_transform1(g._edges))\n",
            "    .nodes(df, 'col_c')\n",
            "    .nodes(my_transform2(g._nodes))\n",
            "    .bind(source='col_a', destination='col_b', node='col_c')\n",
            "    .bind(\n",
            "      point_color='col_a',\n",
            "      point_size='col_b',\n",
            "      point_title='col_c',\n",
            "      point_x='col_d',\n",
            "      point_y='col_e')\n",
            "    .bind(\n",
            "      edge_color='col_m',\n",
            "      edge_weight='col_n',\n",
            "      edge_title='col_o')\n",
            "    .encode_edge_color('timestamp', [\"blue\", \"yellow\", \"red\"], as_continuous=True)\n",
            "    .encode_point_icon('device_type', categorical_mapping={'macbook': 'laptop', ...})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = OpenSearchVectorSearch.from_documents(\n",
        "    docs,\n",
        "    embeddings,\n",
        "    index_name=\"pygraphistry-docs\",  # TODO: use the same index-name used in the ingestion script\n",
        "    # embedding_function=embeddings,\n",
        "    opensearch_url=OS_TOKEN,\n",
        "    engine=\"faiss\",\n",
        "    space_type=\"innerproduct\",\n",
        "    ef_construction=256,\n",
        "    m=48,\n",
        ")\n",
        "\n",
        "# query = \"how\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "8yqv12y26M_A",
        "outputId": "48853daa-fc6f-40c2-e524-e6f3d2933795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "See help(g.dbscan) or help(g.transform_dbscan) for options\n",
            "Quickly configurable\n",
            "Set visual attributes through quick data bindings and set all sorts of URL options. Check out the tutorials on colors, sizes, icons, badges, weighted clustering and sharing controls:\n",
            "  g\n",
            "    .privacy(mode='private', invited_users=[{'email': 'friend1@site.ngo', 'action': '10'}], notify=False)\n",
            "    .edges(df, 'col_a', 'col_b')\n",
            "    .edges(my_transform1(g._edges))\n",
            "    .nodes(df, 'col_c')\n",
            "    .nodes(my_transform2(g._nodes))\n",
            "    .bind(source='col_a', destination='col_b', node='col_c')\n",
            "    .bind(\n",
            "      point_color='col_a',\n",
            "      point_size='col_b',\n",
            "      point_title='col_c',\n",
            "      point_x='col_d',\n",
            "      point_y='col_e')\n",
            "    .bind(\n",
            "      edge_color='col_m',\n",
            "      edge_weight='col_n',\n",
            "      edge_title='col_o')\n",
            "    .encode_edge_color('timestamp', [\"blue\", \"yellow\", \"red\"], as_continuous=True)\n",
            "    .encode_point_icon('device_type', categorical_mapping={'macbook': 'laptop', ...})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sprinkle in OpenAI magic"
      ],
      "metadata": {
        "id": "90tBsp_dAHh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
        "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
        "]"
      ],
      "metadata": {
        "id": "wwQR_T68-VI3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "aVgAeahN-fGp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "_gb9Jtwr-ayl",
        "outputId": "2a1b13be-7bb2-463e-bff7-4cd94b39c12d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_prompt(query: str):\n",
        "    # get top 3 results from knowledge base\n",
        "    results = docsearch.similarity_search(query, k=3)\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt"
      ],
      "metadata": {
        "id": "qCTjpBmY-qtF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"how can i use UMAP in pygraphistry?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "id": "aMUZyBQp6fLV",
        "outputId": "4b69612f-7567-4e28-92ae-f3b79b620ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To use UMAP in PyGraphistry, you can install PyGraphistry with the optional graphistry[ai] dependencies. This will add support for UMAP along with other features like graph autoML and graph neural networks. By combining PyGraphistry with these dependencies, you can easily utilize UMAP for your graph analysis and visualization tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"how can i run gfql in pygraphistry?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "id": "jFyRb3uH-zdJ",
        "outputId": "24808476-2282-4f78-f2bb-4c1010b93111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To run GFQL (Graphistry Frontend Query Language) in PyGraphistry, you can start by creating a graphistry client object and then use the `gfql` method to execute your GFQL queries. Here's a simple example:\n",
            "\n",
            "```python\n",
            "import graphistry\n",
            "\n",
            "# Create a PyGraphistry client\n",
            "graphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\", username=\"your_username\", password=\"your_password\")\n",
            "\n",
            "# Define your GFQL query\n",
            "query = \"YOUR_GFQL_QUERY_HERE\"\n",
            "\n",
            "# Execute the GFQL query\n",
            "result = graphistry.gfql(query)\n",
            "\n",
            "# Display or process the result as needed\n",
            "print(result)\n",
            "```\n",
            "\n",
            "Make sure to replace `\"your_username\"` and `\"your_password\"` with your actual credentials and `\"YOUR_GFQL_QUERY_HERE\"` with your specific GFQL query. This code snippet demonstrates how you can run GFQL queries in PyGraphistry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"will that run easily?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "id": "y8pWFUH9-f30",
        "outputId": "281d6261-46b0-4742-ef33-b2a8670e99c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, it seems like the query is related to the ease of running PyGraphistry for exploring relationships. Yes, PyGraphistry is designed to be easy to install and use. You can install the client in your notebook or web app using pip, connect to a free Graphistry Hub account, or launch your own private GPU server. The installation process is straightforward, and you can choose different data plugins or AI modules based on your requirements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IArVnAmLBV0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "redacre",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}